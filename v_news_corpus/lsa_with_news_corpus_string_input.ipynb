{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned student response:  sample student response here\n",
      "Corpus built and ready for similarity query\n",
      "LSA Score:  0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all imports here \n",
    "from collections import defaultdict\n",
    "from gensim import corpora, models\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from gensim import similarities\n",
    "import time\n",
    "\n",
    "def clean_student_response(response):\n",
    "    \"\"\"\n",
    "    This function cleans student response\n",
    "    this function is called by query_lsa \n",
    "    \"\"\"\n",
    "    response = response.replace('\"', '' ).replace('“', '' ).replace('”', '' ).replace(\"'\", '' ).replace('\\n', ' ').replace('\\r', '').lower()\n",
    "    bad_chars = [';', ':', \".\",\"[\",\"]\",\"#\",'!',\"?\",\"*\", \"/\", \"@\", \"(\", \")\", \"<\", \">\", \"|\", \"-\", \",\", \"&\", \"0\", \"1\", \"2\", \"3\",\"4\", \"5\", \"6\",\"7\",\"8\",\"9\"]\n",
    "    for i in bad_chars :\n",
    "        cleaned_response = response.replace(i, '')\n",
    "        response = cleaned_response\n",
    "    print(\"Cleaned student response: \", cleaned_response)\n",
    "    return cleaned_response\n",
    "\n",
    "# Builds corpus with reference text\n",
    "def build_model_with_ref(corpus_doc_csv): \n",
    "\n",
    "    \"\"\"\n",
    "    This function cleans corpus documents csv from symbols\n",
    "    INPUT: corpus doc as csv \n",
    "    ATTENTION: assumes that corpus is in csv format; cleaned reference text in given txt file (edit later) \n",
    "    tailored for newspaper corpus ie. from \n",
    "    this function is called directly \n",
    "    \"\"\"\n",
    "    \n",
    "    # add in reference text to first row of corpus  \n",
    "    with open('ref_text_jungle.txt', 'r') as file:\n",
    "        ref_text = file.read().replace('\\n', '')\n",
    "\n",
    "    df = pd.read_csv(corpus_doc_csv, engine= 'python')\n",
    "    new_row = pd.DataFrame({'file_name':'ref_text', 'content': ref_text}, index =[0]) \n",
    "    df = pd.concat([new_row, df]).reset_index(drop = True) \n",
    "\n",
    "    # clean corpus documents\n",
    "    documents = df[\"content\"].tolist() \n",
    "    bad_chars = [';', ':', \".\",\"[\",\"]\",\"#\",'!',\"?\",\"*\", \"/\",\"From\", \"@\", \"(\", \")\", \"<\", \">\", \"|\", \"-\", \",\", \"&\", \"0\", \"1\", \"2\", \"3\",\"4\", \"5\", \"6\",\"7\",\"8\",\"9\"]\n",
    "    \n",
    "    for document in documents:\n",
    "#         document.encode('utf-8').strip() # error temporarily addressed \n",
    "        for i in bad_chars :\n",
    "            document = document.replace(i, '')\n",
    "        \n",
    "    # remove common words and tokenize\n",
    "    stoplist = set('for a of the and to in'.split())\n",
    "    texts = [\n",
    "        [word for word in document.lower().split() if word not in stoplist]\n",
    "        for document in documents\n",
    "    ]\n",
    "\n",
    "    # remove words that appear only once\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "\n",
    "    texts = [\n",
    "        [token for token in text if frequency[token] > 1]\n",
    "        for text in texts\n",
    "    ]\n",
    "\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
    "    print(\"Corpus built and ready for similarity query\")\n",
    "    return (dictionary, lsi, corpus)\n",
    "    \n",
    "def query_lsa(response):\n",
    "    \"\"\"\n",
    "    This function cleans queries the lsa model with student response\n",
    "    INPUT: relies on student response function \n",
    "    OUTPUT: lsa_score\n",
    "    this function is called directly \n",
    "    \"\"\"\n",
    "    cleaned_response = clean_student_response(response)\n",
    "    doc = cleaned_response\n",
    "    \n",
    "    model = build_model_with_ref(\"./news_combined_test.csv\")\n",
    "    dictionary = model[0]\n",
    "    lsi = model[1]\n",
    "    corpus = model[2]\n",
    "    \n",
    "    vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "    vec_lsi = lsi[vec_bow]  # convert the query to LSI space\n",
    "\n",
    "    index = similarities.Similarity('workdir/',lsi[corpus],num_features=2)  # transform corpus to LSI space and index it\n",
    "\n",
    "    sims = index[vec_lsi]  # perform a similarity query against the corpus\n",
    "\n",
    "    score_query = list(enumerate(sims))\n",
    "    tuple = score_query[0]\n",
    "    lsa_score = tuple[1]\n",
    "    print(\"LSA Score: \", lsa_score)\n",
    "    return lsa_score\n",
    "\n",
    "query_lsa(\"sample student response, here!1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
